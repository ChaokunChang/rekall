{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your First Dataset\n",
    "\n",
    "In this tutorial, you'll load up your first dataset, and write your first query. At the end of this notebook are also some tips for setting up your own dataset.\n",
    "\n",
    "## The Dataset: Cable TV News\n",
    "\n",
    "We'll be using a few cable TV news videos for the workshop. You can find them at https://olimar.stanford.edu/hdd/rekall_tutorials/workshop/videos/. Go ahead and [check one out](https://olimar.stanford.edu/hdd/rekall_tutorials/workshop/videos/CNNW_20150408_200000_The_Lead_With_Jake_Tapper.mp4) now.\n",
    "\n",
    "### Step 1: Load the videos in a Jupyter notebook\n",
    "\n",
    "Loading up videos in a browser works fine for casual viewing, but we'll want to actually load the data in a Jupyter notebook to write queries and do analysis. We can do this using Vgrid, the visualization side of the Rekall ecosystem.\n",
    "\n",
    "Go ahead and run the next cell to get started - if you installed everything correctly, you should see nine thumbnails. **You can click on a thumbnail to expand the video and play it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913f6ba2d8ba4b34961eb840cb84b2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xe5\\x96Kk\\xe30\\x10\\x80\\xffJ\\xf1yI$E~\\xe5\\xd8\\xc2\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vgrid import VGridSpec, VideoMetadata, VideoBlockFormat\n",
    "from vgrid_jupyter import VGridWidget\n",
    "import urllib3, requests, os\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "VIDEO_COLLECTION_BASEURL = \"http://olimar.stanford.edu/hdd/rekall_tutorials/workshop\"\n",
    "VIDEO_METADATA_FILENAME = \"data/video_meta.json\"\n",
    "\n",
    "req = requests.get(os.path.join(VIDEO_COLLECTION_BASEURL, VIDEO_METADATA_FILENAME), verify=False)\n",
    "video_collection = req.json()\n",
    "\n",
    "video_metadata = [\n",
    "    VideoMetadata(v[\"path\"], v[\"id\"], v[\"fps\"], int(v[\"num_frames\"]), v[\"width\"], v[\"height\"])\n",
    "    for v in video_collection\n",
    "]\n",
    "\n",
    "VIDEO_ENDPOINT = \"http://olimar.stanford.edu/hdd/rekall_tutorials/workshop/videos\"\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = None, video_meta = video_metadata),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did we do?\n",
    "\n",
    "Let's briefly walk through what our code did.\n",
    "\n",
    "To visualize the videos, we need some **basic metadata** about the videos - their width, height, fps, and duration (in terms of number of frames). These were pre-comptued for you, and are stored on our server at http://olimar.stanford.edu/hdd/rekall_tutorials/workshop/data/video_meta.json. **If you're setting up your own dataset, you'll need to compute these info about your videos -- more about that below.**\n",
    "\n",
    "All we do is load this JSON file into Python:\n",
    "\n",
    "```Python\n",
    "req = requests.get(os.path.join(VIDEO_COLLECTION_BASEURL, VIDEO_METADATA_FILENAME), verify=False)\n",
    "video_collection = req.json()\n",
    "```\n",
    "\n",
    "You can inspect the contents of `video_collection` to see what the metadata look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 17458,\n",
       " 'path': 'CNNW_20161218_210000_CNN_Newsroom_With_Fredricka_Whitfield.mp4',\n",
       " 'num_frames': 219440,\n",
       " 'height': 360,\n",
       " 'fps': 59.94,\n",
       " 'width': 640}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_collection[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we created `VideoMetadata` objects from the JSON file, and visualized them using a `VGridWidget`:\n",
    "\n",
    "```Python\n",
    "video_metadata = [\n",
    "    VideoMetadata(v[\"path\"], v[\"id\"], v[\"fps\"], int(v[\"num_frames\"]), v[\"width\"], v[\"height\"])\n",
    "    for v in video_collection\n",
    "]\n",
    "\n",
    "VIDEO_ENDPOINT = \"http://olimar.stanford.edu/hdd/rekall_tutorials/workshop/videos\"\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = None, video_meta = video_metadata),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())\n",
    "```\n",
    "\n",
    "**Notice that we specify the `video_endpoint` when instantiating the VGridSpec.** This tells our web browser where to look for videos. If you're using your own dataset, you may need to use a different video endpoint -- this can be a localhost server on your laptop, or an actual server that you run. Some tips on setting that up are at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load in the outputs of some off-the-shelf networks\n",
    "\n",
    "Now that we have videos loaded in, let's load in the outputs of a face detector and identity classifier. We already ran a face detector and identity classifier over these videos (one frame every three seconds), and have stored the results in a [JSON file](https://olimar.stanford.edu/hdd/rekall_tutorials/workshop/data/faces.json) on our server.\n",
    "\n",
    "Go ahead and run the next cell - this code will:\n",
    "* Load the JSON file from the server\n",
    "* Load the face bounding boxes and identities **into Rekall**\n",
    "* Convert the time units from frame numbers to seconds\n",
    "* Display the face bounding boxes in Vgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103a5da4b4654605a180e648df62def4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xcc\\xbd\\xcd\\x8edA\\x92\\x9d\\xf7*\\xc4\\xac\\x85\\x86\\xff…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rekall imports\n",
    "from rekall import Interval, IntervalSet, IntervalSetMapping, Bounds3D\n",
    "from rekall.stdlib import ingest\n",
    "\n",
    "# Load the JSON file from the server\n",
    "FACES_JSON = \"data/faces.json\"\n",
    "req = requests.get(os.path.join(VIDEO_COLLECTION_BASEURL, FACES_JSON), verify=False)\n",
    "faces_json = req.json()\n",
    "\n",
    "# Load the face bounding boxes into Rekall\n",
    "faces_ism = ingest.ism_from_iterable_with_schema_bounds3D(\n",
    "    faces_json,\n",
    "    ingest.getter_accessor,\n",
    "    {\n",
    "        'key': 'video_id',\n",
    "        't1': 'frame_number', # NOTE that the JSON format has frame timestamps!\n",
    "        't2': 'frame_number',\n",
    "        'x1': 'x1',\n",
    "        'x2': 'x2',\n",
    "        'y1': 'y1',\n",
    "        'y2': 'y2'\n",
    "    },\n",
    "    with_payload = lambda json_obj: json_obj\n",
    ")\n",
    "\n",
    "# Convert from frames to seconds\n",
    "video_meta_by_id = {\n",
    "    vm.id: vm\n",
    "    for vm in video_metadata\n",
    "}\n",
    "\n",
    "faces_ism = faces_ism.map(\n",
    "    lambda face: Interval(\n",
    "        Bounds3D(\n",
    "            # We convert from frames to seconds, and account for temporal downsampling\n",
    "            face['t1'] / video_meta_by_id[face['payload']['video_id']].fps - 1.5,\n",
    "            face['t2'] / video_meta_by_id[face['payload']['video_id']].fps + 1.5,\n",
    "            face['x1'], face['x2'], face['y1'], face['y2']\n",
    "        ),\n",
    "        face['payload']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display in VGrid\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = [\n",
    "        ('faces', faces_ism)\n",
    "    ]),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did we do?\n",
    "\n",
    "Let's look through what we did again.\n",
    "\n",
    "#### Load faces JSON file\n",
    "First, we loaded the faces JSON file:\n",
    "```Python\n",
    "FACES_JSON = \"data/faces.json\"\n",
    "req = requests.get(os.path.join(VIDEO_COLLECTION_BASEURL, FACES_JSON), verify=False)\n",
    "faces_json = req.json()\n",
    "```\n",
    "\n",
    "You can inspect the results to see what this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_id': 19882,\n",
       " 'x1': 0.0,\n",
       " 'y1': 0.0606050528585911,\n",
       " 'x2': 0.158529967069626,\n",
       " 'y2': 0.388492971658707,\n",
       " 'score': 1.0,\n",
       " 'frame_number': 44055,\n",
       " 'gender': 'M',\n",
       " 'gender_score': 1.0,\n",
       " 'identity': 'david m. rodriguez',\n",
       " 'identity_score': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces_json[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the X and Y co-ordinates are frame-relative.\n",
    "\n",
    "#### Load Faces into Rekall\n",
    "\n",
    "Then, we loaded the faces into Rekall:\n",
    "\n",
    "```Python\n",
    "# Load the face bounding boxes into Rekall\n",
    "faces_ism = ingest.ism_from_iterable_with_schema_bounds3D(\n",
    "    faces_json,\n",
    "    ingest.getter_accessor,\n",
    "    {\n",
    "        'key': 'video_id',\n",
    "        't1': 'frame_number', # NOTE that the JSON format has frame timestamps!\n",
    "        't2': 'frame_number',\n",
    "        'x1': 'x1',\n",
    "        'x2': 'x2',\n",
    "        'y1': 'y1',\n",
    "        'y2': 'y2'\n",
    "    },\n",
    "    with_payload = lambda json_obj: json_obj\n",
    ")\n",
    "```\n",
    "\n",
    "This function iterated through our `faces_json` object, and loaded in bounding boxes with times `t1` to `t2` (in frame-time), and X and Y coordinates of `x1`, `x2`, `y1`, `y2`. It also associated each bounding box with the right video using `video_id`, and gave each bounding box a \"payload\" of the JSON object it came from.\n",
    "\n",
    "#### Convert to Seconds\n",
    "\n",
    "Vgrid expects time co-ordinates in terms of seconds, so next we convert the time co-ordinates into seconds. We also accounted for the fact that we only ran face detection once every three seconds.\n",
    "\n",
    "```Python\n",
    "# Convert from frames to seconds\n",
    "video_meta_by_id = {\n",
    "    vm.id: vm\n",
    "    for vm in video_metadata\n",
    "}\n",
    "\n",
    "faces_ism = faces_ism.map(\n",
    "    lambda face: Interval(\n",
    "        Bounds3D(\n",
    "            # We convert from frames to seconds, and account for temporal downsampling\n",
    "            face['t1'] / video_meta_by_id[face['payload']['video_id']].fps - 1.5,\n",
    "            face['t2'] / video_meta_by_id[face['payload']['video_id']].fps + 1.5,\n",
    "            face['x1'], face['x2'], face['y1'], face['y2']\n",
    "        ),\n",
    "        face['payload']\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "#### Display in VGrid\n",
    "\n",
    "Finally, we displayed these bounding boxes in VGrid:\n",
    "\n",
    "```Python\n",
    "# Display in VGrid\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = [\n",
    "        ('faces', faces_ism)\n",
    "    ]),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())\n",
    "```\n",
    "\n",
    "Notice that we pass in a list of tuples to the `imaps` argument of `VideoBlockFormat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.5: Display More Data\n",
    "\n",
    "We can also use a bit more of VGrid's visualization capabilities to display the predicted identity of each face (notice that not all faces have identities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ae70fa4f64492cbb93564efe62d8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xcc\\xbd\\xdb\\x8e$\\xc9\\x91\\xa6\\xf9*\\x05^/\\nz>\\xec\\xe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vgrid import SpatialType_Bbox\n",
    "\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = [\n",
    "        ('faces_with_identities', faces_ism.map(\n",
    "            lambda face: Interval(\n",
    "                face['bounds'],\n",
    "                {\n",
    "                    'spatial_type': SpatialType_Bbox(\n",
    "                        text = (\n",
    "                            face['payload']['identity']\n",
    "                            if face['payload']['identity_score'] > 0.9\n",
    "                            else ''\n",
    "                        )\n",
    "                ), }\n",
    "            )\n",
    "        ))\n",
    "    ]),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualizatio code is the same as before, except now we change the payload of our bounding boxes to display some text if the `identity_score` is high enough:\n",
    "\n",
    "```Python\n",
    "VideoBlockFormat(imaps = [\n",
    "    ('faces_with_identities', faces_ism.map(\n",
    "        lambda face: Interval(\n",
    "            face['bounds'],\n",
    "            {\n",
    "                'spatial_type': SpatialType_Bbox(\n",
    "                    text = (\n",
    "                        face['payload']['identity']\n",
    "                        if face['payload']['identity_score'] > 0.9\n",
    "                        else ''\n",
    "                    )\n",
    "            ), }\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Your First Query!\n",
    "\n",
    "Now let's write your first query - let's look for every detected instance of Jake Tapper's face. With Rekall, this is a simple filter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f663ad2910d4114a2ed26d219b8a5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VGridWidget(vgrid_spec={'compressed': True, 'data': b'x\\x9c\\xcc\\xbd\\xdb\\xaelKR\\xa6\\xf9*(\\xaf[S~>\\xf4e\\xbfB\\xf7…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load up Rekall predicates\n",
    "from rekall.predicates import *\n",
    "\n",
    "jake_tapper = faces_ism.filter(\n",
    "    lambda face: face['payload']['identity'] == 'jake tapper'\n",
    ")\n",
    "\n",
    "# Display in VGrid\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = video_metadata,\n",
    "    vis_format = VideoBlockFormat(imaps = [\n",
    "        ('jake tapper', jake_tapper)\n",
    "    ]),\n",
    "    video_endpoint = VIDEO_ENDPOINT\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Did\n",
    "\n",
    "All we did was filter for face bounding boxes where the detected identity was Jake Tapper:\n",
    "\n",
    "```Python\n",
    "jake_tapper = faces_ism.filter(\n",
    "    lambda face: face['payload']['identity'] == 'jake tapper'\n",
    ")\n",
    "```\n",
    "\n",
    "We'll be writing many queries like this (and much more complex ones) during the workshop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps: Setting up your own dataset\n",
    "\n",
    "If you're planning on bringing your own dataset to the workshop, you'll want to re-create these steps before the workshop. Here's what you'll need to do:\n",
    "1. Make sure your videos are in mp4 (necessary to display in the browser - [ffmpeg](https://askubuntu.com/questions/396883/how-to-simply-convert-video-files-i-e-mkv-to-mp4/396906#396906) is a good tool for this)\n",
    "2. Get the initial metadata - width, height, fps, num_frames for your videos. If you know this already, you can just create the `VideoMetadata` objects yourself! Otherwise, [this script](https://github.com/scanner-research/esperlight/blob/master/create_video_metadata.py) is the one that we use to generate metadata. It relies on `ffmpeg`.\n",
    "3. Generate primitives that you can use to write queries over. There are many options for this - object detections, face detections (like we showed here), pose estimations. As you saw in this tutorial, we have a lot of experience visualizing bounding boxes, but we can also visualize [text data](https://github.com/scanner-research/vgrid/blob/master/examples/05_text_data.py) (designed with caption/event tags in mind) and [poses](https://github.com/scanner-research/vgrid/blob/master/examples/06_keypoints.py) (example uses OpenPose).\n",
    "\n",
    "## Displaying videos in your browser\n",
    "\n",
    "To actually display the videos in your browser, you'll need a simple server to host your videos (and then point Vgrid to that endpoint). The simplest option is just to spin up the server on your host machine.\n",
    "\n",
    "For example, you can navigate to the folder where you have stored the videos, and run a command like this:\n",
    "```bash\n",
    "python -m http.server\n",
    "```\n",
    "Then, you should be able to navigate to http://localhost:8000 and see all the files in that folder (including your videos). Then you'll just need to change the `video_endpoint` in your `VGridSpec`:\n",
    "```Python\n",
    "vgrid_spec = VGridSpec(\n",
    "    video_meta = ...,\n",
    "    vis_format = ...,\n",
    "    video_endpoint = 'http://localhost:8000'\n",
    ")\n",
    "VGridWidget(vgrid_spec = vgrid_spec.to_json_compressed())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
